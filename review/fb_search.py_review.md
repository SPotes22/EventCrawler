<!-- hash:bfa3279fdea5bae37a1262f2ff7d9460fed247fca31222dfaf1c49440f5ce3ac -->
<!-- stack:generic -->
# üìã Code Review: fb_search.py

**Archivo:** `fb_search.py`
**Stack:** GENERIC
**Fecha:** 2025-08-17 18:10:47
**L√≠neas de c√≥digo:** 229
**Tokens utilizados:** 4257

---

```python
import time
import csv
import os
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ============================
# CONFIG
# ============================
load_dotenv()
FB_EMAIL = os.getenv("FB_EMAIL")
FB_PASSWORD = os.getenv("FB_PASSWORD")

OUTPUT_FILE = "eventos_fb.csv"
KEYWORDS = ["concierto", "feria", "conmemoriacion", "tributo", "fumaton"]
CITIES = ["pereira", "santarosa", "dosquebradas"]

WATCHDOG_TIMEOUT = 30

# ============================
# SELENIUM SETUP
# ============================
def create_driver():
    chrome_options = Options()
    chrome_options.add_argument("--disable-notifications")
    chrome_options.add_argument("--start-maximized")
    return webdriver.Chrome(options=chrome_options)

driver = create_driver()
wait = WebDriverWait(driver, 15)


# ============================
# LOGIN
# ============================
def login_facebook():
    driver.get("https://www.facebook.com/")
    try:
        email_input = wait.until(EC.presence_of_element_located((By.ID, "email")))
        pass_input = driver.find_element(By.ID, "pass")
        email_input.send_keys(FB_EMAIL)
        pass_input.send_keys(FB_PASSWORD)
        pass_input.send_keys(Keys.RETURN)
        print("‚úÖ Login exitoso")
        time.sleep(8)

        # Si pide 2FA ‚Üí esperar intervenci√≥n manual
        try:
            wait.until(
                EC.presence_of_element_located(
                    (By.XPATH, "//span[contains(text(),'Inicio') or contains(text(),'Home')]")
                ),
                60,
            )
        except:
            print("‚ö†Ô∏è Esperando que completes 2FA manualmente...")
            time.sleep(60)
    except Exception as e:
        print("‚ö†Ô∏è Ya estabas logueado o fall√≥ login:", e)


# ============================
# SCROLL
# ============================
def scroll_until_end():
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
        time.sleep(3)

        try:
            driver.find_element(By.XPATH, "//span[contains(text(),'End of results')]")
            print("üìç Fin de resultados detectado")
            break
        except:
            pass

        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            print("üìç No crece el scroll, detenemos")
            break
        last_height = new_height


# ============================
# SCRAPERS
# ============================
def scrape_layout_b():
    eventos = []
    blocks = driver.find_elements(By.XPATH, "//div[contains(@class,'x1qjc9v5')]")
    print(f"üîé Layout B detectado ‚Üí {len(blocks)} bloques encontrados")
    for idx, block in enumerate(blocks):
        try:
            title_element = block.find_element(By.XPATH, ".//a[@aria-label]")
            title = title_element.get_attribute("aria-label")

            link_element = block.find_element(By.XPATH, ".//a[@href and contains(@href, 'eid=')]")
            link = link_element.get_attribute("href")

            eventos.append({"title": title, "link": link})
            print(f"‚úÖ Evento {idx + 1}: {title}")
        except Exception as e:
            print(f"‚ö†Ô∏è Error al extraer evento {idx + 1}: {e}")
    return eventos


def scrape_layout_a():
    eventos = []
    blocks = driver.find_elements(By.XPATH, "//div[contains(@class,'du4w35lb')]")
    print(f"üîé Layout A detectado ‚Üí {len(blocks)} bloques encontrados")
    for idx, block in enumerate(blocks):
        try:
            title_element = block.find_element(By.XPATH, ".//a[@aria-label]")
            title = title_element.get_attribute("aria-label")

            link_element = block.find_element(By.XPATH, ".//a[@href and contains(@href, 'eid=')]")
            link = link_element.get_attribute("href")

            eventos.append({"title": title, "link": link})
            print(f"‚úÖ Evento {idx + 1}: {title}")
        except Exception as e:
            print(f"‚ö†Ô∏è Error al extraer evento {idx + 1}: {e}")
    return eventos


def scrape_events():
    # Intenta el Layout B primero, si no encuentra nada, intenta Layout A
    eventos = scrape_layout_b()
    if not eventos:
        eventos = scrape_layout_a()
    return eventos


# ============================
# SEARCH
# ============================
def search_and_scrape(keyword, city):
    search_query = f"{keyword} en {city}"
    print(f"üîé Buscando: {search_query}")
    driver.get(f"https://www.facebook.com/search/events/?q={search_query}")
    time.sleep(5)  # Espera a que cargue la p√°gina

    scroll_until_end()
    eventos = scrape_events()
    return eventos


# ============================
# CSV
# ============================
def write_to_csv(eventos, filename=OUTPUT_FILE):
    with open(filename, "a", newline="", encoding="utf-8") as csvfile:
        fieldnames = ["title", "link", "keyword", "city"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        # Escribir encabezado solo si el archivo est√° vac√≠o
        if csvfile.tell() == 0:
            writer.writeheader()

        for evento in eventos:
            writer.writerow(evento)


# ============================
# MAIN
# ============================
def main():
    login_facebook()
    all_events = []
    for keyword in KEYWORDS:
        for city in CITIES:
            events = search_and_scrape(keyword, city)
            # A√±adir keyword y city a cada evento para el CSV
            for event in events:
                event["keyword"] = keyword
                event["city"] = city
            all_events.extend(events)

    write_to_csv(all_events)
    print(f"‚úÖ Se encontraron {len(all_events)} eventos y se guardaron en {OUTPUT_FILE}")
    driver.quit()


if __name__ == "__main__":
    main()
```

### Resumen
El c√≥digo es un script de scraping web que utiliza Selenium para automatizar la b√∫squeda de eventos en Facebook basados en una lista de palabras clave y ciudades. Inicia sesi√≥n en Facebook, realiza b√∫squedas, extrae informaci√≥n de los eventos encontrados (t√≠tulo y enlace) y guarda los resultados en un archivo CSV.

### Funcionalidades principales
1.  **Login**: Inicia sesi√≥n en Facebook usando credenciales almacenadas en variables de entorno.
2.  **B√∫squeda**: Realiza b√∫squedas de eventos en Facebook combinando palabras clave y ciudades.
3.  **Scroll infinito**: Simula el scroll de la p√°gina para cargar m√°s resultados.
4.  **Scraping**: Extrae el t√≠tulo y el enlace de cada evento encontrado. Tiene l√≥gica para adaptarse a dos posibles layouts de Facebook (A y B).
5.  **Guardado en CSV**: Guarda los resultados en un archivo CSV, incluyendo el t√≠tulo del evento, el enlace, la palabra clave y la ciudad.

### Arquitectura y patrones (espec√≠ficos para generic)
*   **Modularidad**: El c√≥digo est√° dividido en funciones l√≥gicas (login, scroll, scraping, search, csv writing), lo que facilita su mantenimiento y reutilizaci√≥n.
*   **Configuraci√≥n centralizada**: Las configuraciones (credenciales, palabras clave, ciudades, nombre del archivo de salida) se definen al principio del script, lo que facilita su modificaci√≥n.  Usa `dotenv` para gestionar las credenciales, una pr√°ctica recomendada.
*   **Manejo de errores**: Utiliza bloques `try...except` para manejar posibles errores durante el login y el scraping.
*   **Adaptaci√≥n a cambios en el layout**: Implementa dos scrapers (`scrape_layout_a` y `scrape_layout_b`) y una l√≥gica para determinar cu√°l usar (`scrape_events`). Esto intenta mitigar el impacto de los cambios en la estructura HTML de Facebook.
*   **Separaci√≥n de responsabilidades:** Cada funci√≥n tiene una tarea espec√≠fica.  Por ejemplo, `create_driver` solo crea el driver de Selenium, `login_facebook` solo se encarga del login, etc.

### Posibles mejoras
1.  **Manejo de excepciones m√°s robusto**:  En los bloques `except`, ser√≠a √∫til registrar el error en un archivo de log y continuar con el proceso, en lugar de simplemente imprimirlo. Esto permitir√≠a una mejor depuraci√≥n y evitar√≠a que el script se detenga por errores inesperados.
2.  **Refactorizaci√≥n de `scrape_events`**:  La funci√≥n `scrape_events` es un ejemplo de  *Feature Envy*.  Deber√≠a haber una funci√≥n general para encontrar los bloques, y las funciones `scrape_layout_a` y `scrape_layout_b` deber√≠an enfocarse en la extracci√≥n de datos *dentro* de esos bloques.
3.  **Mejorar la detecci√≥n del "fin de resultados"**: El scroll infinito actual se basa en detectar el "End of results" o en detectar que la altura de la p√°gina no cambia.  Estos m√©todos no son muy confiables.  Podr√≠a ser mejor implementar un sistema de paginaci√≥n (si Facebook lo permite) o un sistema que detecte si se han cargado nuevos eventos en un cierto periodo de tiempo.
4.  **Paralelizaci√≥n**: El script podr√≠a ser m√°s r√°pido si se paralelizaran las b√∫squedas (por ejemplo, usando `multiprocessing` o `threading`). Esto permitir√≠a buscar en m√∫ltiples ciudades y con m√∫ltiples palabras clave al mismo tiempo.
5.  **Uso de proxies**:  Para evitar ser bloqueado por Facebook, se podr√≠a usar una lista de proxies rotatorios.
6.  **Configuraci√≥n m√°s flexible**: Considerar usar un archivo de configuraci√≥n (YAML o JSON) para almacenar las palabras clave, ciudades y otras opciones de configuraci√≥n. Esto facilitar√≠a la modificaci√≥n de la configuraci√≥n sin necesidad de editar el c√≥digo.
7.  **Implementar un sistema de reintentos**: Si una b√∫squeda falla (por ejemplo, debido a un error de red), se podr√≠a implementar un sistema de reintentos autom√°ticos.
8.  **Agregar logging**: Implementar un sistema de logging adecuado en lugar de solo `print` statements. Esto har√≠a que la depuraci√≥n y el monitoreo del script sean mucho m√°s f√°ciles.
9.  **M√°s inteligencia en la detecci√≥n de layouts**: Facebook puede cambiar su layout con frecuencia.  Una alternativa a tener scrapers espec√≠ficos para layouts A y B ser√≠a usar un modelo de machine learning para detectar el layout y adaptar el scraping din√°micamente.  Esto ser√≠a una mejora mucho m√°s compleja, pero tambi√©n mucho m√°s robusta.
10. **Watchdog Timer**: Incluir un watchdog timer para la espera manual del 2FA. Si el usuario no completa el 2FA en un tiempo razonable (definido en la variable `WATCHDOG_TIMEOUT`), el script debe terminar, previniendo bloqueos indefinidos.

### Problemas de seguridad
1.  **Almacenamiento de credenciales**: Aunque las credenciales se cargan desde variables de entorno, es importante asegurarse de que estas variables de entorno no se almacenen en el c√≥digo fuente ni se compartan accidentalmente. Es crucial tener cuidado al versionar el c√≥digo y al desplegarlo.
2.  **Posible bloqueo por Facebook**: Facebook puede detectar y bloquear el script si se realizan demasiadas solicitudes en un corto per√≠odo de tiempo.  El uso de proxies y la implementaci√≥n de un retraso aleatorio entre las solicitudes pueden ayudar a mitigar este riesgo.
3.  **Vulnerabilidades de Selenium**:  Es importante mantener Selenium y el driver del navegador actualizados para evitar posibles vulnerabilidades de seguridad.

### Recomendaciones para GENERIC
1.  **Abstracci√≥n de scrapers**:  Crear una clase base abstracta `Scraper` con m√©todos para `login`, `scroll`, `extract_data`, y una clase hija para cada sitio web que se desea scrapear.  Esto facilita la adici√≥n de nuevos sitios web.
2.  **Sistema de plugins**:  Implementar un sistema de plugins para los scrapers.  Esto permitir√≠a a los usuarios agregar sus propios scrapers sin necesidad de modificar el c√≥digo principal.
3.  **Interfaz de usuario**:  Proporcionar una interfaz de usuario (GUI o CLI) para configurar las b√∫squedas, las credenciales y otras opciones.
4.  **Sistema de colas de mensajes**:  Utilizar un sistema de colas de mensajes (por ejemplo, RabbitMQ o Celery) para distribuir las tareas de scraping entre m√∫ltiples m√°quinas.  Esto permitir√≠a escalar el script para scrapear grandes cantidades de datos.
5.  **Orquestaci√≥n de contenedores**:  Empaquetar el script en un contenedor Docker y utilizar un sistema de orquestaci√≥n de contenedores (por ejemplo, Kubernetes) para gestionarlo. Esto facilita el despliegue y la gesti√≥n del script en un entorno de producci√≥n.
6.  **Monitorizaci√≥n**:  Implementar un sistema de monitorizaci√≥n para supervisar el script y detectar posibles problemas.  Esto podr√≠a incluir la monitorizaci√≥n del uso de la CPU, el uso de la memoria, el tiempo de ejecuci√≥n, y la tasa de errores.
7.  **Sistema de alertas**:  Implementar un sistema de alertas para notificar a los usuarios cuando ocurran errores o cuando el script detecte algo inusual.
8.  **Versionado de la API de scraping**:  Almacenar el XPATH selector en una base de datos con versionado. Crear una capa API que gestione el scraper para evitar despliegues en caso de que Facebook cambie el DOM.
9. **Integraci√≥n con servicios de resoluci√≥n de CAPTCHAs**: Implementar la resoluci√≥n autom√°tica de CAPTCHAs utilizando servicios como 2Captcha o Anti-Captcha para evitar interrupciones en el proceso de scraping.

En resumen, el c√≥digo proporcionado es un buen punto de partida para un proyecto de scraping web.  Implementando las mejoras y recomendaciones mencionadas, se puede convertir en una soluci√≥n robusta, escalable y f√°cil de mantener.


---
‚úÖ Sin cambios significativos - √öltima revisi√≥n 2025-08-17 18:13:08

---
‚úÖ Sin cambios significativos - √öltima revisi√≥n 2025-08-17 18:13:37